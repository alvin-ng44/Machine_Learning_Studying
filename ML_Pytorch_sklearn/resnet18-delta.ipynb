{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "377dc768-6e18-444c-bcc8-741eb27b20f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms, models\n",
    "from torch.utils.data import random_split, DataLoader\n",
    "from torch.optim.lr_scheduler import StepLR, SequentialLR, LambdaLR, OneCycleLR\n",
    "import os\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6461f4fa-326a-4ca1-a108-e1e4e08c4cce",
   "metadata": {},
   "outputs": [],
   "source": [
    "lambda_data = '/mnt/d/Users/Admin/Projects/Machine_Learning/data'\n",
    "delta_data = '/workspace/alvin/Machine_Learning_Studying/data'\n",
    "data_lst = [lambda_data, delta_data]\n",
    "data_pth = data_lst[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "09fdeca5-901b-45ee-8be8-366ace8e3040",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_transforms = {\n",
    "    \"train\" : transforms.Compose([\n",
    "        transforms.Resize(size = (224, 224)), # resize to 224 by 224 for resnet\n",
    "        # transforms.CenterCrop(size = (224, 224)),\n",
    "        transforms.Grayscale(num_output_channels = 3), # this converts grayscale to rgb channels\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean = [0.485, 0.456, 0.406], std = [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    \"test\" : transforms.Compose([\n",
    "        transforms.Resize(size = (224, 224)), # resize to 224 by 224 for resnet\n",
    "        # transforms.CenterCrop(size = (224, 224)),\n",
    "        transforms.Grayscale(num_output_channels = 3), # this converts grayscale to rgb channels\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean = [0.485, 0.456, 0.406], std = [0.229, 0.224, 0.225])\n",
    "    ])\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "413fd109-c338-4fe5-a5b7-2cc173ea884b",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_map = {\n",
    "    0: \"T-Shirt\",\n",
    "    1: \"Trouser\",\n",
    "    2: \"Pullover\",\n",
    "    3: \"Dress\",\n",
    "    4: \"Coat\",\n",
    "    5: \"Sandal\",\n",
    "    6: \"Shirt\",\n",
    "    7: \"Sneaker\",\n",
    "    8: \"Bag\",\n",
    "    9: \"Ankle Boot\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e4756577-edc5-4cf1-ae45-4520d7bc5ac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = datasets.FashionMNIST(\n",
    "    root = data_pth,\n",
    "    train = True,\n",
    "    download = True,\n",
    "    transform = data_transforms[\"train\"]\n",
    ")\n",
    "\n",
    "train_ds, val_ds = random_split(training_data, [50_000, 10_000])\n",
    "\n",
    "ds_dict = {\"train\" : train_ds, \"val\" : val_ds}\n",
    "dataset_sizes = {\"train\" : len(train_ds), \"val\" : len(val_ds)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "49ab5a2c-2c4b-4c1d-ae80-001e412ad483",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset FashionMNIST\n",
       "    Number of datapoints: 60000\n",
       "    Root location: /workspace/alvin/Machine_Learning_Studying/data\n",
       "    Split: Train\n",
       "    StandardTransform\n",
       "Transform: Compose(\n",
       "               Resize(size=(224, 224), interpolation=bilinear, max_size=None, antialias=True)\n",
       "               Grayscale(num_output_channels=3)\n",
       "               ToTensor()\n",
       "               Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
       "           )"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8f5f6972-1858-4b9e-9445-d7b67082a22d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloaders = {x : DataLoader(ds_dict[x], batch_size = 32, num_workers = 2, shuffle = True) for x in ds_dict.keys()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "596a9297-ca6d-4a32-bb20-53869aeb0f12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load pre-trained model\n",
    "model = models.resnet18(weights = \"DEFAULT\")\n",
    "# Replace final layer for the number of classes\n",
    "model.fc = nn.Linear(model.fc.in_features, len(labels_map))\n",
    "\n",
    "# Freeze all layers except final layer\n",
    "# final layer is responsible for classification\n",
    "for name, param in model.named_parameters():\n",
    "    if \"fc\" in name:\n",
    "        # unfreeze the fc layers\n",
    "        # this means we are only training the fc layers\n",
    "        param.requires_grad = True \n",
    "    else:\n",
    "        param.requires_grad = False\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss() # most common used nn for classification problems\n",
    "# optimizer = optim.SGD(model.parameters(), lr = 0.01)\n",
    "# scheduler = StepLR(optimizer, step_size = 10, gamma = 0.1) # this multiplies lr every 10 epoch by 0.1\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr = 0.001)\n",
    "\n",
    "# def warmup_lambda(epoch):\n",
    "#     if epoch < 20:\n",
    "#         # scale from 0.5 (0.1/0.2) up to 1.0\n",
    "#         return 0.5 + (epoch / 20) * 0.5\n",
    "#     return 1.0\n",
    "\n",
    "# scheduler1 = LambdaLR(optimizer, lr_lambda = warmup_lambda)\n",
    "# scheduler2 = StepLR(optimizer, step_size = 4, gamma = 0.5)\n",
    "# scheduler = SequentialLR(optimizer, schedulers = [scheduler1, scheduler2], milestones = [20])\n",
    "\n",
    "scheduler = OneCycleLR(optimizer, max_lr = 0.0125, epochs = 50, steps_per_epoch = len(dataloaders[\"train\"]))\n",
    "\n",
    "# move model to GPU\n",
    "device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8cae5e4c-c4ff-4f17-b431-e7482610c8d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.12/multiprocessing/queues.py\", line 264, in _feed\n",
      "    obj = _ForkingPickler.dumps(obj)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/lib/python3.12/multiprocessing/reduction.py\", line 51, in dumps\n",
      "    cls(buf, protocol).dump(obj)\n",
      "  File \"/opt/py_venv/lib/python3.12/site-packages/torch/multiprocessing/reductions.py\", line 618, in reduce_storage\n",
      "    fd, size = storage._share_fd_cpu_()\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/py_venv/lib/python3.12/site-packages/torch/storage.py\", line 451, in wrapper\n",
      "    return fn(self, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/py_venv/lib/python3.12/site-packages/torch/storage.py\", line 526, in _share_fd_cpu_\n",
      "    return super()._share_fd_cpu_(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "RuntimeError: unable to write to file </torch_245115_1502081546_1>: No space left on device (28)\n",
      "ERROR: Unexpected bus error encountered in worker. This might be caused by insufficient shared memory (shm).\n",
      "\u0000\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Training loops\n",
    "num_epochs = 50\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"Epoch {epoch}\")\n",
    "    for phase in [\"train\", \"val\"]:\n",
    "        if phase == \"train\":\n",
    "            model.train()\n",
    "        else:\n",
    "            model.eval()\n",
    "\n",
    "        running_loss = 0.0\n",
    "        running_corrects = 0 # correct predictions\n",
    "\n",
    "        for inputs, labels in dataloaders[phase]:\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad() # clear the gradient from previous iteration\n",
    "\n",
    "            with torch.set_grad_enabled(phase == \"train\"):\n",
    "                outputs = model(inputs)\n",
    "                _, preds = torch.max(outputs, 1)\n",
    "                loss = criterion(outputs, labels) # check if output and labels match\n",
    "\n",
    "                if phase == \"train\":\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                    scheduler.step() # scheduler here if OneCycleLR\n",
    "\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "        epoch_loss = running_loss / dataset_sizes[phase]\n",
    "        epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
    "\n",
    "        print(f\"{phase} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}\")\n",
    "\n",
    "    # scheduler.step()\n",
    "    print(f\"Epoch {epoch+1} LR: {scheduler.get_last_lr()[0]:.10f}\")\n",
    "\n",
    "print(\"Training complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "833108fa-fd0a-4dbf-9eb4-ca7fd56e24df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model\n",
    "torch.save(model.state_dict(), \"/mnt/d/Users/Admin/Projects/Machine_Learning/weights/fashionmnist_epoch_50_resnet18_adaptive_lambda.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bbddf23-3c6c-49ec-a103-a1656088e626",
   "metadata": {},
   "source": [
    "## Classification on Test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "da579d28-efa0-4b55-b2ce-84ddde45ddf8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Linear(in_features=512, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_model = models.resnet18(weights = None) # dont load ImageNet Weights\n",
    "new_model.fc = nn.Linear(new_model.fc.in_features, len(labels_map))\n",
    "\n",
    "# Load your trained weights\n",
    "new_model.load_state_dict(torch.load(\n",
    "    \"/mnt/d/Users/Admin/Projects/Machine_Learning/weights/fashionmnist_epoch_50_resnet18_adaptive_lambda.pth\",\n",
    "    map_location=device\n",
    "))\n",
    "\n",
    "new_model = new_model.to(device)\n",
    "new_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "43be0237-2be2-4d6c-8e5d-d658fdc5513d",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = datasets.FashionMNIST(\n",
    "    root = data_pth,\n",
    "    train = False,\n",
    "    download = True,\n",
    "    transform = data_transforms[\"test\"]\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_data,\n",
    "    batch_size = 128,\n",
    "    num_workers = 8,\n",
    "    shuffle = False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "61261f8c-254d-4d6c-80a6-fcb8c77b574c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.8669\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        outputs = new_model(inputs)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "\n",
    "        correct += torch.sum(preds == labels).item()\n",
    "        total += labels.size(0)\n",
    "\n",
    "test_acc = correct / total\n",
    "print(f\"Test Accuracy: {test_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "eec9dbc2-9a2b-4f3a-b1a3-6756b9749379",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image 1 \n",
      " Pred: Ankle Boot, True: Ankle Boot\n",
      "image 2 \n",
      " Pred: Pullover, True: Pullover\n",
      "image 3 \n",
      " Pred: Trouser, True: Trouser\n",
      "image 4 \n",
      " Pred: Trouser, True: Trouser\n",
      "image 5 \n",
      " Pred: Shirt, True: Shirt\n",
      "image 6 \n",
      " Pred: Trouser, True: Trouser\n",
      "image 7 \n",
      " Pred: Coat, True: Coat\n",
      "image 8 \n",
      " Pred: Shirt, True: Shirt\n",
      "image 9 \n",
      " Pred: Sandal, True: Sandal\n",
      "image 10 \n",
      " Pred: Sneaker, True: Sneaker\n",
      "image 11 \n",
      " Pred: Coat, True: Coat\n",
      "image 12 \n",
      " Pred: Sandal, True: Sandal\n",
      "image 13 \n",
      " Pred: Sneaker, True: Sneaker\n",
      "image 14 \n",
      " Pred: Dress, True: Dress\n",
      "image 15 \n",
      " Pred: Coat, True: Coat\n",
      "image 16 \n",
      " Pred: Trouser, True: Trouser\n",
      "image 17 \n",
      " Pred: Pullover, True: Pullover\n",
      "image 18 \n",
      " Pred: T-Shirt, True: Coat\n",
      "image 19 \n",
      " Pred: Bag, True: Bag\n",
      "image 20 \n",
      " Pred: T-Shirt, True: T-Shirt\n",
      "image 21 \n",
      " Pred: Pullover, True: Pullover\n",
      "image 22 \n",
      " Pred: Sneaker, True: Sandal\n",
      "image 23 \n",
      " Pred: Sneaker, True: Sneaker\n",
      "image 24 \n",
      " Pred: Sandal, True: Ankle Boot\n",
      "image 25 \n",
      " Pred: Trouser, True: Trouser\n",
      "image 26 \n",
      " Pred: Shirt, True: Coat\n",
      "image 27 \n",
      " Pred: Shirt, True: Shirt\n",
      "image 28 \n",
      " Pred: T-Shirt, True: T-Shirt\n",
      "image 29 \n",
      " Pred: Ankle Boot, True: Ankle Boot\n",
      "image 30 \n",
      " Pred: Shirt, True: Dress\n",
      "image 31 \n",
      " Pred: Bag, True: Bag\n",
      "image 32 \n",
      " Pred: Bag, True: Bag\n",
      "image 33 \n",
      " Pred: Dress, True: Dress\n",
      "image 34 \n",
      " Pred: Dress, True: Dress\n",
      "image 35 \n",
      " Pred: Bag, True: Bag\n",
      "image 36 \n",
      " Pred: T-Shirt, True: T-Shirt\n",
      "image 37 \n",
      " Pred: Sneaker, True: Sneaker\n",
      "image 38 \n",
      " Pred: Sandal, True: Sandal\n",
      "image 39 \n",
      " Pred: Sneaker, True: Sneaker\n",
      "image 40 \n",
      " Pred: Ankle Boot, True: Ankle Boot\n",
      "image 41 \n",
      " Pred: T-Shirt, True: Shirt\n",
      "image 42 \n",
      " Pred: Trouser, True: Trouser\n",
      "image 43 \n",
      " Pred: Dress, True: Dress\n",
      "image 44 \n",
      " Pred: Sneaker, True: Sneaker\n",
      "image 45 \n",
      " Pred: Coat, True: Shirt\n",
      "image 46 \n",
      " Pred: Sneaker, True: Sneaker\n",
      "image 47 \n",
      " Pred: Pullover, True: Pullover\n",
      "image 48 \n",
      " Pred: Trouser, True: Trouser\n",
      "image 49 \n",
      " Pred: Coat, True: Pullover\n",
      "image 50 \n",
      " Pred: Shirt, True: Pullover\n",
      "image 51 \n",
      " Pred: Coat, True: Coat\n",
      "image 52 \n",
      " Pred: Coat, True: Coat\n",
      "image 53 \n",
      " Pred: Sandal, True: Sandal\n",
      "image 54 \n",
      " Pred: Bag, True: Bag\n",
      "image 55 \n",
      " Pred: Pullover, True: Pullover\n",
      "image 56 \n",
      " Pred: Pullover, True: Pullover\n",
      "image 57 \n",
      " Pred: Bag, True: Bag\n",
      "image 58 \n",
      " Pred: Coat, True: Coat\n",
      "image 59 \n",
      " Pred: Bag, True: Bag\n",
      "image 60 \n",
      " Pred: T-Shirt, True: T-Shirt\n",
      "image 61 \n",
      " Pred: Sneaker, True: Sneaker\n",
      "image 62 \n",
      " Pred: Sneaker, True: Sneaker\n",
      "image 63 \n",
      " Pred: Bag, True: Bag\n",
      "image 64 \n",
      " Pred: Sandal, True: Sandal\n",
      "image 65 \n",
      " Pred: Trouser, True: Trouser\n",
      "image 66 \n",
      " Pred: Trouser, True: Trouser\n",
      "image 67 \n",
      " Pred: Coat, True: Pullover\n",
      "image 68 \n",
      " Pred: Dress, True: Dress\n",
      "image 69 \n",
      " Pred: Sneaker, True: Ankle Boot\n",
      "image 70 \n",
      " Pred: Bag, True: Bag\n",
      "image 71 \n",
      " Pred: Sneaker, True: Sneaker\n",
      "image 72 \n",
      " Pred: T-Shirt, True: T-Shirt\n",
      "image 73 \n",
      " Pred: Pullover, True: Pullover\n",
      "image 74 \n",
      " Pred: Shirt, True: Shirt\n",
      "image 75 \n",
      " Pred: Pullover, True: Pullover\n",
      "image 76 \n",
      " Pred: Dress, True: Dress\n",
      "image 77 \n",
      " Pred: Trouser, True: Trouser\n",
      "image 78 \n",
      " Pred: Pullover, True: Pullover\n",
      "image 79 \n",
      " Pred: Bag, True: Bag\n",
      "image 80 \n",
      " Pred: Coat, True: Coat\n",
      "image 81 \n",
      " Pred: Trouser, True: Trouser\n",
      "image 82 \n",
      " Pred: Bag, True: Bag\n",
      "image 83 \n",
      " Pred: Sandal, True: Sandal\n",
      "image 84 \n",
      " Pred: Ankle Boot, True: Ankle Boot\n",
      "image 85 \n",
      " Pred: Sandal, True: Sandal\n",
      "image 86 \n",
      " Pred: T-Shirt, True: T-Shirt\n",
      "image 87 \n",
      " Pred: Dress, True: Dress\n",
      "image 88 \n",
      " Pred: Pullover, True: Pullover\n",
      "image 89 \n",
      " Pred: T-Shirt, True: T-Shirt\n",
      "image 90 \n",
      " Pred: Shirt, True: Shirt\n",
      "image 91 \n",
      " Pred: Sandal, True: Sandal\n",
      "image 92 \n",
      " Pred: Dress, True: Dress\n",
      "image 93 \n",
      " Pred: Shirt, True: Shirt\n",
      "image 94 \n",
      " Pred: Sneaker, True: Sneaker\n",
      "image 95 \n",
      " Pred: Trouser, True: Trouser\n",
      "image 96 \n",
      " Pred: Bag, True: Bag\n",
      "image 97 \n",
      " Pred: T-Shirt, True: T-Shirt\n",
      "image 98 \n",
      " Pred: Trouser, True: Trouser\n",
      "image 99 \n",
      " Pred: Pullover, True: Coat\n",
      "image 100 \n",
      " Pred: Pullover, True: Pullover\n"
     ]
    }
   ],
   "source": [
    "for i in range(100):\n",
    "    image, label = test_data[i]\n",
    "    image_batch = image.unsqueeze(0).to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = new_model(image_batch)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "\n",
    "    pred_class = preds.item()\n",
    "    \n",
    "    # plt.imshow(image[0].squeeze(), cmap = \"gray\")\n",
    "    # plt.title(f\"image {i+1}\")\n",
    "    print(f\"image {i+1} \\n Pred: {labels_map[pred_class]}, True: {labels_map[label]}\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7612c4f3-3c15-407e-bece-e3a302566a8c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
