{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "377dc768-6e18-444c-bcc8-741eb27b20f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms, models\n",
    "from torch.utils.data import random_split, DataLoader\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "import os\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6461f4fa-326a-4ca1-a108-e1e4e08c4cce",
   "metadata": {},
   "outputs": [],
   "source": [
    "lambda_data = '/mnt/d/Users/Admin/Projects/Machine_Learning/data'\n",
    "delta_data = '/workspace/alvin/Machine_Learning_Studying/data'\n",
    "data_lst = [lambda_data, delta_data]\n",
    "data_pth = data_lst[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "09fdeca5-901b-45ee-8be8-366ace8e3040",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_transforms = {\n",
    "    \"train\" : transforms.Compose([\n",
    "        transforms.Resize(size = (224, 224)), # resize to 224 by 224 for resnet\n",
    "        # transforms.CenterCrop(size = (224, 224)),\n",
    "        transforms.Grayscale(num_output_channels = 3), # this converts grayscale to rgb channels\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean = [0.485, 0.456, 0.406], std = [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    \"test\" : transforms.Compose([\n",
    "        transforms.Resize(size = (224, 224)), # resize to 224 by 224 for resnet\n",
    "        # transforms.CenterCrop(size = (224, 224)),\n",
    "        transforms.Grayscale(num_output_channels = 3), # this converts grayscale to rgb channels\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean = [0.485, 0.456, 0.406], std = [0.229, 0.224, 0.225])\n",
    "    ])\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "413fd109-c338-4fe5-a5b7-2cc173ea884b",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_map = {\n",
    "    0: \"T-Shirt\",\n",
    "    1: \"Trouser\",\n",
    "    2: \"Pullover\",\n",
    "    3: \"Dress\",\n",
    "    4: \"Coat\",\n",
    "    5: \"Sandal\",\n",
    "    6: \"Shirt\",\n",
    "    7: \"Sneaker\",\n",
    "    8: \"Bag\",\n",
    "    9: \"Ankle Boot\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e4756577-edc5-4cf1-ae45-4520d7bc5ac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = datasets.FashionMNIST(\n",
    "    root = data_pth,\n",
    "    train = True,\n",
    "    download = True,\n",
    "    transform = data_transforms[\"train\"]\n",
    ")\n",
    "\n",
    "train_ds, val_ds = random_split(training_data, [50_000, 10_000])\n",
    "\n",
    "ds_dict = {\"train\" : train_ds, \"val\" : val_ds}\n",
    "dataset_sizes = {\"train\" : len(train_ds), \"val\" : len(val_ds)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "49ab5a2c-2c4b-4c1d-ae80-001e412ad483",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset FashionMNIST\n",
       "    Number of datapoints: 60000\n",
       "    Root location: /mnt/d/Users/Admin/Projects/Machine_Learning/data\n",
       "    Split: Train\n",
       "    StandardTransform\n",
       "Transform: Compose(\n",
       "               Resize(size=(224, 224), interpolation=bilinear, max_size=None, antialias=True)\n",
       "               Grayscale(num_output_channels=3)\n",
       "               ToTensor()\n",
       "               Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
       "           )"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8f5f6972-1858-4b9e-9445-d7b67082a22d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloaders = {x : DataLoader(ds_dict[x], batch_size = 2**7, num_workers = 8, shuffle = True) for x in ds_dict.keys()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "596a9297-ca6d-4a32-bb20-53869aeb0f12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load pre-trained model\n",
    "model = models.resnet18(weights = \"DEFAULT\")\n",
    "# Replace final layer for the number of classes\n",
    "model.fc = nn.Linear(model.fc.in_features, len(labels_map))\n",
    "\n",
    "# Freeze all layers except final layer\n",
    "# final layer is responsible for classification\n",
    "for name, param in model.named_parameters():\n",
    "    if \"fc\" in name:\n",
    "        # unfreeze the fc layers\n",
    "        # this means we are only training the fc layers\n",
    "        param.requires_grad = True \n",
    "    else:\n",
    "        param.requires_grad = False\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss() # most common used nn for classification problems\n",
    "optimizer = optim.SGD(model.parameters(), lr = 0.01)\n",
    "scheduler = StepLR(optimizer, step_size = 10, gamma = 0.1) # this multiplies lr every 10 epoch by 0.1\n",
    "\n",
    "# move model to GPU\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8cae5e4c-c4ff-4f17-b431-e7482610c8d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "train Loss: 0.9354 Acc: 0.7369\n",
      "val Loss: 0.6154 Acc: 0.8095\n",
      "Epoch 1 LR: 0.0100000000\n",
      "Epoch 2/50\n",
      "train Loss: 0.5729 Acc: 0.8186\n",
      "val Loss: 0.5263 Acc: 0.8275\n",
      "Epoch 2 LR: 0.0100000000\n",
      "Epoch 3/50\n",
      "train Loss: 0.5120 Acc: 0.8307\n",
      "val Loss: 0.4910 Acc: 0.8330\n",
      "Epoch 3 LR: 0.0100000000\n",
      "Epoch 4/50\n",
      "train Loss: 0.4816 Acc: 0.8377\n",
      "val Loss: 0.4682 Acc: 0.8405\n",
      "Epoch 4 LR: 0.0100000000\n",
      "Epoch 5/50\n",
      "train Loss: 0.4612 Acc: 0.8431\n",
      "val Loss: 0.4509 Acc: 0.8456\n",
      "Epoch 5 LR: 0.0100000000\n",
      "Epoch 6/50\n",
      "train Loss: 0.4479 Acc: 0.8458\n",
      "val Loss: 0.4452 Acc: 0.8482\n",
      "Epoch 6 LR: 0.0100000000\n",
      "Epoch 7/50\n",
      "train Loss: 0.4368 Acc: 0.8495\n",
      "val Loss: 0.4300 Acc: 0.8534\n",
      "Epoch 7 LR: 0.0100000000\n",
      "Epoch 8/50\n",
      "train Loss: 0.4282 Acc: 0.8520\n",
      "val Loss: 0.4242 Acc: 0.8531\n",
      "Epoch 8 LR: 0.0100000000\n",
      "Epoch 9/50\n",
      "train Loss: 0.4229 Acc: 0.8532\n",
      "val Loss: 0.4177 Acc: 0.8555\n",
      "Epoch 9 LR: 0.0100000000\n",
      "Epoch 10/50\n",
      "train Loss: 0.4140 Acc: 0.8555\n",
      "val Loss: 0.4116 Acc: 0.8567\n",
      "Epoch 10 LR: 0.0010000000\n",
      "Epoch 11/50\n",
      "train Loss: 0.4097 Acc: 0.8574\n",
      "val Loss: 0.4105 Acc: 0.8573\n",
      "Epoch 11 LR: 0.0010000000\n",
      "Epoch 12/50\n",
      "train Loss: 0.4077 Acc: 0.8573\n",
      "val Loss: 0.4113 Acc: 0.8573\n",
      "Epoch 12 LR: 0.0010000000\n",
      "Epoch 13/50\n",
      "train Loss: 0.4081 Acc: 0.8560\n",
      "val Loss: 0.4102 Acc: 0.8574\n",
      "Epoch 13 LR: 0.0010000000\n",
      "Epoch 14/50\n",
      "train Loss: 0.4085 Acc: 0.8568\n",
      "val Loss: 0.4100 Acc: 0.8576\n",
      "Epoch 14 LR: 0.0010000000\n",
      "Epoch 15/50\n",
      "train Loss: 0.4053 Acc: 0.8590\n",
      "val Loss: 0.4095 Acc: 0.8581\n",
      "Epoch 15 LR: 0.0010000000\n",
      "Epoch 16/50\n",
      "train Loss: 0.4062 Acc: 0.8580\n",
      "val Loss: 0.4085 Acc: 0.8577\n",
      "Epoch 16 LR: 0.0010000000\n",
      "Epoch 17/50\n",
      "train Loss: 0.4056 Acc: 0.8585\n",
      "val Loss: 0.4078 Acc: 0.8578\n",
      "Epoch 17 LR: 0.0010000000\n",
      "Epoch 18/50\n",
      "train Loss: 0.4054 Acc: 0.8571\n",
      "val Loss: 0.4080 Acc: 0.8576\n",
      "Epoch 18 LR: 0.0010000000\n",
      "Epoch 19/50\n",
      "train Loss: 0.4057 Acc: 0.8585\n",
      "val Loss: 0.4072 Acc: 0.8579\n",
      "Epoch 19 LR: 0.0010000000\n",
      "Epoch 20/50\n",
      "train Loss: 0.4052 Acc: 0.8584\n",
      "val Loss: 0.4060 Acc: 0.8581\n",
      "Epoch 20 LR: 0.0001000000\n",
      "Epoch 21/50\n",
      "train Loss: 0.4033 Acc: 0.8591\n",
      "val Loss: 0.4065 Acc: 0.8585\n",
      "Epoch 21 LR: 0.0001000000\n",
      "Epoch 22/50\n",
      "train Loss: 0.4040 Acc: 0.8591\n",
      "val Loss: 0.4069 Acc: 0.8579\n",
      "Epoch 22 LR: 0.0001000000\n",
      "Epoch 23/50\n",
      "train Loss: 0.4025 Acc: 0.8590\n",
      "val Loss: 0.4070 Acc: 0.8590\n",
      "Epoch 23 LR: 0.0001000000\n",
      "Epoch 24/50\n",
      "train Loss: 0.4038 Acc: 0.8588\n",
      "val Loss: 0.4061 Acc: 0.8598\n",
      "Epoch 24 LR: 0.0001000000\n",
      "Epoch 25/50\n",
      "train Loss: 0.4033 Acc: 0.8589\n",
      "val Loss: 0.4070 Acc: 0.8590\n",
      "Epoch 25 LR: 0.0001000000\n",
      "Epoch 26/50\n",
      "train Loss: 0.4028 Acc: 0.8584\n",
      "val Loss: 0.4068 Acc: 0.8591\n",
      "Epoch 26 LR: 0.0001000000\n",
      "Epoch 27/50\n",
      "train Loss: 0.4032 Acc: 0.8590\n",
      "val Loss: 0.4064 Acc: 0.8583\n",
      "Epoch 27 LR: 0.0001000000\n",
      "Epoch 28/50\n",
      "train Loss: 0.4025 Acc: 0.8586\n",
      "val Loss: 0.4065 Acc: 0.8591\n",
      "Epoch 28 LR: 0.0001000000\n",
      "Epoch 29/50\n",
      "train Loss: 0.4034 Acc: 0.8583\n",
      "val Loss: 0.4063 Acc: 0.8591\n",
      "Epoch 29 LR: 0.0001000000\n",
      "Epoch 30/50\n",
      "train Loss: 0.4035 Acc: 0.8587\n",
      "val Loss: 0.4074 Acc: 0.8579\n",
      "Epoch 30 LR: 0.0000100000\n",
      "Epoch 31/50\n",
      "train Loss: 0.4040 Acc: 0.8577\n",
      "val Loss: 0.4073 Acc: 0.8584\n",
      "Epoch 31 LR: 0.0000100000\n",
      "Epoch 32/50\n",
      "train Loss: 0.4035 Acc: 0.8580\n",
      "val Loss: 0.4066 Acc: 0.8582\n",
      "Epoch 32 LR: 0.0000100000\n",
      "Epoch 33/50\n",
      "train Loss: 0.4034 Acc: 0.8575\n",
      "val Loss: 0.4068 Acc: 0.8600\n",
      "Epoch 33 LR: 0.0000100000\n",
      "Epoch 34/50\n",
      "train Loss: 0.4037 Acc: 0.8591\n",
      "val Loss: 0.4063 Acc: 0.8581\n",
      "Epoch 34 LR: 0.0000100000\n",
      "Epoch 35/50\n",
      "train Loss: 0.4022 Acc: 0.8594\n",
      "val Loss: 0.4068 Acc: 0.8588\n",
      "Epoch 35 LR: 0.0000100000\n",
      "Epoch 36/50\n",
      "train Loss: 0.4026 Acc: 0.8586\n",
      "val Loss: 0.4080 Acc: 0.8577\n",
      "Epoch 36 LR: 0.0000100000\n",
      "Epoch 37/50\n",
      "train Loss: 0.4040 Acc: 0.8575\n",
      "val Loss: 0.4058 Acc: 0.8582\n",
      "Epoch 37 LR: 0.0000100000\n",
      "Epoch 38/50\n",
      "train Loss: 0.4038 Acc: 0.8587\n",
      "val Loss: 0.4063 Acc: 0.8585\n",
      "Epoch 38 LR: 0.0000100000\n",
      "Epoch 39/50\n",
      "train Loss: 0.4035 Acc: 0.8581\n",
      "val Loss: 0.4060 Acc: 0.8592\n",
      "Epoch 39 LR: 0.0000100000\n",
      "Epoch 40/50\n",
      "train Loss: 0.4026 Acc: 0.8592\n",
      "val Loss: 0.4071 Acc: 0.8584\n",
      "Epoch 40 LR: 0.0000010000\n",
      "Epoch 41/50\n",
      "train Loss: 0.4035 Acc: 0.8579\n",
      "val Loss: 0.4068 Acc: 0.8585\n",
      "Epoch 41 LR: 0.0000010000\n",
      "Epoch 42/50\n",
      "train Loss: 0.4029 Acc: 0.8583\n",
      "val Loss: 0.4068 Acc: 0.8581\n",
      "Epoch 42 LR: 0.0000010000\n",
      "Epoch 43/50\n",
      "train Loss: 0.4038 Acc: 0.8590\n",
      "val Loss: 0.4071 Acc: 0.8587\n",
      "Epoch 43 LR: 0.0000010000\n",
      "Epoch 44/50\n",
      "train Loss: 0.4043 Acc: 0.8575\n",
      "val Loss: 0.4079 Acc: 0.8592\n",
      "Epoch 44 LR: 0.0000010000\n",
      "Epoch 45/50\n",
      "train Loss: 0.4030 Acc: 0.8585\n",
      "val Loss: 0.4069 Acc: 0.8588\n",
      "Epoch 45 LR: 0.0000010000\n",
      "Epoch 46/50\n",
      "train Loss: 0.4026 Acc: 0.8591\n",
      "val Loss: 0.4061 Acc: 0.8582\n",
      "Epoch 46 LR: 0.0000010000\n",
      "Epoch 47/50\n",
      "train Loss: 0.4019 Acc: 0.8591\n",
      "val Loss: 0.4070 Acc: 0.8588\n",
      "Epoch 47 LR: 0.0000010000\n",
      "Epoch 48/50\n",
      "train Loss: 0.4039 Acc: 0.8591\n",
      "val Loss: 0.4066 Acc: 0.8587\n",
      "Epoch 48 LR: 0.0000010000\n",
      "Epoch 49/50\n",
      "train Loss: 0.4030 Acc: 0.8580\n",
      "val Loss: 0.4066 Acc: 0.8595\n",
      "Epoch 49 LR: 0.0000010000\n",
      "Epoch 50/50\n",
      "train Loss: 0.4033 Acc: 0.8587\n",
      "val Loss: 0.4063 Acc: 0.8576\n",
      "Epoch 50 LR: 0.0000001000\n",
      "Training complete!\n"
     ]
    }
   ],
   "source": [
    "# Training loops\n",
    "num_epochs = 50\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "    for phase in [\"train\", \"val\"]:\n",
    "        if phase == \"train\":\n",
    "            model.train()\n",
    "        else:\n",
    "            model.eval()\n",
    "\n",
    "        running_loss = 0.0\n",
    "        running_corrects = 0 # correct predictions\n",
    "\n",
    "        for inputs, labels in dataloaders[phase]:\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad() # clear the gradient from previous iteration\n",
    "\n",
    "            with torch.set_grad_enabled(phase == \"train\"):\n",
    "                outputs = model(inputs)\n",
    "                _, preds = torch.max(outputs, 1)\n",
    "                loss = criterion(outputs, labels) # check if output and labels match\n",
    "\n",
    "                if phase == \"train\":\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "        epoch_loss = running_loss / dataset_sizes[phase]\n",
    "        epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
    "\n",
    "        print(f\"{phase} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}\")\n",
    "\n",
    "    scheduler.step()\n",
    "    print(f\"Epoch {epoch+1} LR: {scheduler.get_last_lr()[0]:.10f}\")\n",
    "\n",
    "print(\"Training complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "833108fa-fd0a-4dbf-9eb4-ca7fd56e24df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model\n",
    "torch.save(model.state_dict(), \"/mnt/d/Users/Admin/Projects/Machine_Learning/weights/fashionmnist_epoch_50_resnet18_lambda.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bbddf23-3c6c-49ec-a103-a1656088e626",
   "metadata": {},
   "source": [
    "## Classification on Test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "da579d28-efa0-4b55-b2ce-84ddde45ddf8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Linear(in_features=512, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_model = models.resnet18(weights = None) # dont load ImageNet Weights\n",
    "new_model.fc = nn.Linear(new_model.fc.in_features, len(labels_map))\n",
    "\n",
    "# Load your trained weights\n",
    "new_model.load_state_dict(torch.load(\n",
    "    \"/mnt/d/Users/Admin/Projects/Machine_Learning/weights/fashionmnist_epoch_50_resnet18_lambda.pth\",\n",
    "    map_location=device\n",
    "))\n",
    "\n",
    "new_model = new_model.to(device)\n",
    "new_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "43be0237-2be2-4d6c-8e5d-d658fdc5513d",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = datasets.FashionMNIST(\n",
    "    root = data_pth,\n",
    "    train = False,\n",
    "    download = True,\n",
    "    transform = data_transforms[\"test\"]\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_data,\n",
    "    batch_size = 128,\n",
    "    num_workers = 8,\n",
    "    shuffle = False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "61261f8c-254d-4d6c-80a6-fcb8c77b574c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.8559\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        outputs = new_model(inputs)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "\n",
    "        correct += torch.sum(preds == labels).item()\n",
    "        total += labels.size(0)\n",
    "\n",
    "test_acc = correct / total\n",
    "print(f\"Test Accuracy: {test_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "eec9dbc2-9a2b-4f3a-b1a3-6756b9749379",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image 1 \n",
      " Pred: Ankle Boot, True: Ankle Boot\n",
      "image 2 \n",
      " Pred: Pullover, True: Pullover\n",
      "image 3 \n",
      " Pred: Trouser, True: Trouser\n",
      "image 4 \n",
      " Pred: Trouser, True: Trouser\n",
      "image 5 \n",
      " Pred: Shirt, True: Shirt\n",
      "image 6 \n",
      " Pred: Trouser, True: Trouser\n",
      "image 7 \n",
      " Pred: Coat, True: Coat\n",
      "image 8 \n",
      " Pred: Shirt, True: Shirt\n",
      "image 9 \n",
      " Pred: Sandal, True: Sandal\n",
      "image 10 \n",
      " Pred: Sneaker, True: Sneaker\n",
      "image 11 \n",
      " Pred: Coat, True: Coat\n",
      "image 12 \n",
      " Pred: Sandal, True: Sandal\n",
      "image 13 \n",
      " Pred: Sneaker, True: Sneaker\n",
      "image 14 \n",
      " Pred: Dress, True: Dress\n",
      "image 15 \n",
      " Pred: Coat, True: Coat\n",
      "image 16 \n",
      " Pred: Trouser, True: Trouser\n",
      "image 17 \n",
      " Pred: Pullover, True: Pullover\n",
      "image 18 \n",
      " Pred: T-Shirt, True: Coat\n",
      "image 19 \n",
      " Pred: Bag, True: Bag\n",
      "image 20 \n",
      " Pred: T-Shirt, True: T-Shirt\n",
      "image 21 \n",
      " Pred: T-Shirt, True: Pullover\n",
      "image 22 \n",
      " Pred: Sneaker, True: Sandal\n",
      "image 23 \n",
      " Pred: Sneaker, True: Sneaker\n",
      "image 24 \n",
      " Pred: Sandal, True: Ankle Boot\n",
      "image 25 \n",
      " Pred: Trouser, True: Trouser\n",
      "image 26 \n",
      " Pred: Shirt, True: Coat\n",
      "image 27 \n",
      " Pred: Shirt, True: Shirt\n",
      "image 28 \n",
      " Pred: T-Shirt, True: T-Shirt\n",
      "image 29 \n",
      " Pred: Ankle Boot, True: Ankle Boot\n",
      "image 30 \n",
      " Pred: Dress, True: Dress\n",
      "image 31 \n",
      " Pred: Bag, True: Bag\n",
      "image 32 \n",
      " Pred: Bag, True: Bag\n",
      "image 33 \n",
      " Pred: Dress, True: Dress\n",
      "image 34 \n",
      " Pred: Dress, True: Dress\n",
      "image 35 \n",
      " Pred: Bag, True: Bag\n",
      "image 36 \n",
      " Pred: T-Shirt, True: T-Shirt\n",
      "image 37 \n",
      " Pred: Sneaker, True: Sneaker\n",
      "image 38 \n",
      " Pred: Sandal, True: Sandal\n",
      "image 39 \n",
      " Pred: Sneaker, True: Sneaker\n",
      "image 40 \n",
      " Pred: Ankle Boot, True: Ankle Boot\n",
      "image 41 \n",
      " Pred: T-Shirt, True: Shirt\n",
      "image 42 \n",
      " Pred: Trouser, True: Trouser\n",
      "image 43 \n",
      " Pred: Dress, True: Dress\n",
      "image 44 \n",
      " Pred: Sneaker, True: Sneaker\n",
      "image 45 \n",
      " Pred: Coat, True: Shirt\n",
      "image 46 \n",
      " Pred: Sneaker, True: Sneaker\n",
      "image 47 \n",
      " Pred: Pullover, True: Pullover\n",
      "image 48 \n",
      " Pred: Trouser, True: Trouser\n",
      "image 49 \n",
      " Pred: Coat, True: Pullover\n",
      "image 50 \n",
      " Pred: Shirt, True: Pullover\n",
      "image 51 \n",
      " Pred: Coat, True: Coat\n",
      "image 52 \n",
      " Pred: Shirt, True: Coat\n",
      "image 53 \n",
      " Pred: Sandal, True: Sandal\n",
      "image 54 \n",
      " Pred: Bag, True: Bag\n",
      "image 55 \n",
      " Pred: Pullover, True: Pullover\n",
      "image 56 \n",
      " Pred: Pullover, True: Pullover\n",
      "image 57 \n",
      " Pred: Bag, True: Bag\n",
      "image 58 \n",
      " Pred: Coat, True: Coat\n",
      "image 59 \n",
      " Pred: Bag, True: Bag\n",
      "image 60 \n",
      " Pred: T-Shirt, True: T-Shirt\n",
      "image 61 \n",
      " Pred: Sneaker, True: Sneaker\n",
      "image 62 \n",
      " Pred: Sneaker, True: Sneaker\n",
      "image 63 \n",
      " Pred: Bag, True: Bag\n",
      "image 64 \n",
      " Pred: Sandal, True: Sandal\n",
      "image 65 \n",
      " Pred: Trouser, True: Trouser\n",
      "image 66 \n",
      " Pred: Trouser, True: Trouser\n",
      "image 67 \n",
      " Pred: Coat, True: Pullover\n",
      "image 68 \n",
      " Pred: Dress, True: Dress\n",
      "image 69 \n",
      " Pred: Sneaker, True: Ankle Boot\n",
      "image 70 \n",
      " Pred: Bag, True: Bag\n",
      "image 71 \n",
      " Pred: Sneaker, True: Sneaker\n",
      "image 72 \n",
      " Pred: T-Shirt, True: T-Shirt\n",
      "image 73 \n",
      " Pred: Pullover, True: Pullover\n",
      "image 74 \n",
      " Pred: T-Shirt, True: Shirt\n",
      "image 75 \n",
      " Pred: Pullover, True: Pullover\n",
      "image 76 \n",
      " Pred: Dress, True: Dress\n",
      "image 77 \n",
      " Pred: Trouser, True: Trouser\n",
      "image 78 \n",
      " Pred: Pullover, True: Pullover\n",
      "image 79 \n",
      " Pred: Bag, True: Bag\n",
      "image 80 \n",
      " Pred: Coat, True: Coat\n",
      "image 81 \n",
      " Pred: Trouser, True: Trouser\n",
      "image 82 \n",
      " Pred: Bag, True: Bag\n",
      "image 83 \n",
      " Pred: Sandal, True: Sandal\n",
      "image 84 \n",
      " Pred: Ankle Boot, True: Ankle Boot\n",
      "image 85 \n",
      " Pred: Sandal, True: Sandal\n",
      "image 86 \n",
      " Pred: T-Shirt, True: T-Shirt\n",
      "image 87 \n",
      " Pred: Dress, True: Dress\n",
      "image 88 \n",
      " Pred: Pullover, True: Pullover\n",
      "image 89 \n",
      " Pred: T-Shirt, True: T-Shirt\n",
      "image 90 \n",
      " Pred: Shirt, True: Shirt\n",
      "image 91 \n",
      " Pred: Sandal, True: Sandal\n",
      "image 92 \n",
      " Pred: Dress, True: Dress\n",
      "image 93 \n",
      " Pred: Shirt, True: Shirt\n",
      "image 94 \n",
      " Pred: Sneaker, True: Sneaker\n",
      "image 95 \n",
      " Pred: Trouser, True: Trouser\n",
      "image 96 \n",
      " Pred: Bag, True: Bag\n",
      "image 97 \n",
      " Pred: T-Shirt, True: T-Shirt\n",
      "image 98 \n",
      " Pred: Trouser, True: Trouser\n",
      "image 99 \n",
      " Pred: Pullover, True: Coat\n",
      "image 100 \n",
      " Pred: Pullover, True: Pullover\n"
     ]
    }
   ],
   "source": [
    "for i in range(100):\n",
    "    image, label = test_data[i]\n",
    "    image_batch = image.unsqueeze(0).to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = new_model(image_batch)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "\n",
    "    pred_class = preds.item()\n",
    "    \n",
    "    # plt.imshow(image[0].squeeze(), cmap = \"gray\")\n",
    "    # plt.title(f\"image {i+1}\")\n",
    "    print(f\"image {i+1} \\n Pred: {labels_map[pred_class]}, True: {labels_map[label]}\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7612c4f3-3c15-407e-bece-e3a302566a8c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
